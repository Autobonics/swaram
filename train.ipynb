{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from gloss_dataset import GlossDataset\n",
    "from gloss_model import GlossModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for training\n",
    "gd = GlossDataset()\n",
    "input_size = gd[0][0].shape[1]\n",
    "class_no = len(gd.classes)\n",
    "input_size, class_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlossModel(\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (relu): ReLU()\n",
       "  (lstm1): LSTM(1596, 128, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.15, inplace=False)\n",
       "  (lstm2): LSTM(128, 64, batch_first=True)\n",
       "  (dropout2): Dropout(p=0.15, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dropout3): Dropout(p=0.15, inplace=False)\n",
       "  (fc2): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide input and class size\n",
    "model = GlossModel(input_size, class_no)\n",
    "optim = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sumamry writer\n",
    "writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing and training dataLoader from single dataset using random_split\n",
    "# and also set training epoch\n",
    "split_ratio = 0.8\n",
    "batch_size = 3\n",
    "train_size = int(split_ratio*len(gd))\n",
    "test_size = len(gd)-train_size\n",
    "train_data, test_data = random_split(gd, [train_size, test_size])\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "epoch = 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/epoch : (3.87121844291687, 0)\n",
      "Loss/epoch : (3.7425785064697266, 100)\n",
      "Loss/epoch : (3.1884632110595703, 200)\n",
      "Loss/epoch : (3.1667633056640625, 300)\n",
      "Loss/epoch : (3.1616437435150146, 400)\n",
      "Loss/epoch : (3.158705711364746, 500)\n",
      "Loss/epoch : (3.1590282917022705, 600)\n",
      "Loss/epoch : (3.1574461460113525, 700)\n",
      "Loss/epoch : (3.15726637840271, 800)\n",
      "Loss/epoch : (3.1575164794921875, 900)\n",
      "Loss/epoch : (3.157027006149292, 1000)\n",
      "Loss/epoch : (3.157036542892456, 1100)\n",
      "Loss/epoch : (3.156921625137329, 1200)\n",
      "Loss/epoch : (3.156505584716797, 1300)\n",
      "Loss/epoch : (3.1566762924194336, 1400)\n",
      "Loss/epoch : (3.156435012817383, 1500)\n",
      "Loss/epoch : (3.15664005279541, 1600)\n",
      "Loss/epoch : (3.156458616256714, 1700)\n",
      "Loss/epoch : (3.1564083099365234, 1800)\n",
      "Loss/epoch : (3.1564056873321533, 1900)\n",
      "Loss/epoch : (3.156407117843628, 2000)\n",
      "Loss/epoch : (3.1564228534698486, 2100)\n",
      "Loss/epoch : (3.156440496444702, 2200)\n",
      "Loss/epoch : (3.1563901901245117, 2300)\n",
      "Loss/epoch : (3.1563761234283447, 2400)\n",
      "Loss/epoch : (3.156385660171509, 2500)\n",
      "Loss/epoch : (3.156377077102661, 2600)\n",
      "Loss/epoch : (3.1563875675201416, 2700)\n",
      "Loss/epoch : (3.1563823223114014, 2800)\n",
      "Loss/epoch : (3.1563808917999268, 2900)\n",
      "Loss/epoch : (3.156374216079712, 3000)\n",
      "Loss/epoch : (3.1563827991485596, 3100)\n",
      "Loss/epoch : (3.1563758850097656, 3200)\n",
      "Loss/epoch : (3.1563751697540283, 3300)\n",
      "Loss/epoch : (3.156378984451294, 3400)\n",
      "Loss/epoch : (3.156377077102661, 3500)\n",
      "Loss/epoch : (3.15637469291687, 3600)\n",
      "Loss/epoch : (3.1563854217529297, 3700)\n",
      "Loss/epoch : (3.156374931335449, 3800)\n",
      "Loss/epoch : (3.1563751697540283, 3900)\n",
      "Loss/epoch : (3.156373977661133, 4000)\n"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "model.train()\n",
    "for i in range(epoch+1):\n",
    "    for x_train, y_train in train_dl:\n",
    "        optim.zero_grad()\n",
    "        out = model(x_train)\n",
    "        loss = loss_fn(out, y_train)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    writer.add_scalar(\"Loss/epoch\", loss.item(), i)\n",
    "    if (i % 100 == 0):\n",
    "        print(f\"Loss/epoch : {loss.item(),i}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 1.0\n",
      "Test Loss : 3.1563732624053955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Test set\n",
    "model.eval()\n",
    "tot_loss = 0\n",
    "tp = 0\n",
    "tot_test = 0\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_dl:\n",
    "        out = model(x_test)\n",
    "        loss = loss_fn(out, y_train)\n",
    "        tot_loss += loss\n",
    "        y_pred = torch.argmax(out[2])\n",
    "        y_test = torch.argmax(y_test)\n",
    "        if y_pred == y_test:\n",
    "            tp += 1\n",
    "        tot_test += 1\n",
    "\n",
    "# Much more data required for training and test set\n",
    "acc = tp/tot_test\n",
    "loss = tot_loss/tot_test\n",
    "print(f\"Test Acc {acc}\\nTest Loss : {loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, \"swaram_lstm.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swaram-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
