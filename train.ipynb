{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from gloss_dataset import GlossDataset\n",
    "from gloss_model import GlossModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for training\n",
    "gd = GlossDataset()\n",
    "input_size = gd[0][0].shape[1]\n",
    "class_no = len(gd.classes)\n",
    "input_size, class_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlossModel(\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (relu): ReLU()\n",
       "  (lstm1): LSTM(1596, 128, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.15, inplace=False)\n",
       "  (lstm2): LSTM(128, 64, batch_first=True)\n",
       "  (dropout2): Dropout(p=0.15, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dropout3): Dropout(p=0.15, inplace=False)\n",
       "  (fc2): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide input and class size\n",
    "model = GlossModel(input_size, class_no)\n",
    "optim = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sumamry writer\n",
    "writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing and training dataLoader from single dataset using random_split\n",
    "# and also set training epoch\n",
    "split_ratio = 0.8\n",
    "batch_size = 4\n",
    "train_size = int(split_ratio*len(gd))\n",
    "test_size = len(gd)-train_size\n",
    "train_data, test_data = random_split(gd, [train_size, test_size])\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "epoch=4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/epoch : (3.871016025543213, 0)\n",
      "Loss/epoch : (3.7347261905670166, 100)\n",
      "Loss/epoch : (3.172196626663208, 200)\n",
      "Loss/epoch : (3.175344467163086, 300)\n",
      "Loss/epoch : (3.1616132259368896, 400)\n",
      "Loss/epoch : (3.1587371826171875, 500)\n",
      "Loss/epoch : (3.1569461822509766, 600)\n",
      "Loss/epoch : (3.156630754470825, 700)\n",
      "Loss/epoch : (3.15665340423584, 800)\n",
      "Loss/epoch : (3.1568446159362793, 900)\n",
      "Loss/epoch : (3.1565518379211426, 1000)\n",
      "Loss/epoch : (3.1566264629364014, 1100)\n",
      "Loss/epoch : (3.156419277191162, 1200)\n",
      "Loss/epoch : (3.156524658203125, 1300)\n",
      "Loss/epoch : (3.1565184593200684, 1400)\n",
      "Loss/epoch : (3.156419277191162, 1500)\n",
      "Loss/epoch : (3.1564266681671143, 1600)\n",
      "Loss/epoch : (3.156437635421753, 1700)\n",
      "Loss/epoch : (3.1566691398620605, 1800)\n",
      "Loss/epoch : (3.1563968658447266, 1900)\n",
      "Loss/epoch : (3.156400203704834, 2000)\n",
      "Loss/epoch : (3.156373977661133, 2100)\n",
      "Loss/epoch : (3.156388282775879, 2200)\n",
      "Loss/epoch : (3.1564462184906006, 2300)\n",
      "Loss/epoch : (3.1563820838928223, 2400)\n",
      "Loss/epoch : (3.1563849449157715, 2500)\n",
      "Loss/epoch : (3.156379222869873, 2600)\n",
      "Loss/epoch : (3.1563773155212402, 2700)\n",
      "Loss/epoch : (3.156376361846924, 2800)\n",
      "Loss/epoch : (3.1563854217529297, 2900)\n",
      "Loss/epoch : (3.1563732624053955, 3000)\n",
      "Loss/epoch : (3.156374931335449, 3100)\n",
      "Loss/epoch : (3.156374216079712, 3200)\n",
      "Loss/epoch : (3.1563758850097656, 3300)\n",
      "Loss/epoch : (3.156385898590088, 3400)\n",
      "Loss/epoch : (3.156383752822876, 3500)\n",
      "Loss/epoch : (3.1563732624053955, 3600)\n",
      "Loss/epoch : (3.1563735008239746, 3700)\n",
      "Loss/epoch : (3.156374931335449, 3800)\n",
      "Loss/epoch : (3.1563732624053955, 3900)\n",
      "Loss/epoch : (3.156374454498291, 4000)\n"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "for i in range(epoch+1):\n",
    "    for x_train, y_train in train_dl:\n",
    "        optim.zero_grad()\n",
    "        out = model(x_train)\n",
    "        loss = loss_fn(out, y_train)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    writer.add_scalar(\"Loss/epoch\", loss.item(), i)\n",
    "    if (i % 100 == 0):\n",
    "        print(f\"Loss/epoch : {loss.item(),i}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, \"swaram_lstm.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swaram-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
