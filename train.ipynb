{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from gloss_dataset import GlossDataset\n",
    "from gloss_model import GlossModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for training\n",
    "gd = GlossDataset()\n",
    "input_size = gd[0][0].shape[1]\n",
    "class_no = len(gd.classes)\n",
    "input_size, class_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide input and class size\n",
    "model = GlossModel(input_size, class_no)\n",
    "device = model.device\n",
    "model.to(device)\n",
    "optim = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Device : \",model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sumamry writer\n",
    "writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing and training dataLoader from single dataset using random_split\n",
    "# and also set training epoch\n",
    "split_ratio = 0.8\n",
    "batch_size = 1\n",
    "train_size = int(split_ratio*len(gd))\n",
    "test_size = len(gd)-train_size\n",
    "train_data, test_data = random_split(gd, [train_size, test_size])\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size,\n",
    "                      shuffle=True, pin_memory=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size,\n",
    "                     shuffle=True, pin_memory=False)\n",
    "epoch = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.6115\n",
      "Epoch 11/1000, Loss: 1.6067\n",
      "Epoch 21/1000, Loss: 1.5970\n",
      "Epoch 31/1000, Loss: 1.5803\n",
      "Epoch 41/1000, Loss: 1.5973\n",
      "Epoch 51/1000, Loss: 1.5505\n",
      "Epoch 61/1000, Loss: 1.5817\n",
      "Epoch 71/1000, Loss: 1.5106\n",
      "Epoch 81/1000, Loss: 1.4839\n",
      "Epoch 91/1000, Loss: 1.4625\n",
      "Epoch 101/1000, Loss: 1.4436\n",
      "Epoch 111/1000, Loss: 1.4320\n",
      "Epoch 121/1000, Loss: 1.3984\n",
      "Epoch 131/1000, Loss: 1.3820\n",
      "Epoch 141/1000, Loss: 1.3711\n",
      "Epoch 151/1000, Loss: 1.3631\n",
      "Epoch 161/1000, Loss: 1.3310\n",
      "Epoch 171/1000, Loss: 1.3589\n",
      "Epoch 181/1000, Loss: 1.3003\n",
      "Epoch 191/1000, Loss: 1.2843\n",
      "Epoch 201/1000, Loss: 1.2774\n",
      "Epoch 211/1000, Loss: 1.2866\n",
      "Epoch 221/1000, Loss: 1.2628\n",
      "Epoch 231/1000, Loss: 1.2403\n",
      "Epoch 241/1000, Loss: 1.2347\n",
      "Epoch 251/1000, Loss: 1.2443\n",
      "Epoch 261/1000, Loss: 1.1774\n",
      "Epoch 271/1000, Loss: 1.2141\n",
      "Epoch 281/1000, Loss: 1.1223\n",
      "Epoch 291/1000, Loss: 1.1327\n",
      "Epoch 301/1000, Loss: 1.1163\n",
      "Epoch 311/1000, Loss: 1.0900\n",
      "Epoch 321/1000, Loss: 1.0724\n",
      "Epoch 331/1000, Loss: 1.0658\n",
      "Epoch 341/1000, Loss: 1.0300\n",
      "Epoch 351/1000, Loss: 1.1852\n",
      "Epoch 361/1000, Loss: 1.0502\n",
      "Epoch 371/1000, Loss: 0.9948\n",
      "Epoch 381/1000, Loss: 1.0114\n",
      "Epoch 391/1000, Loss: 1.0210\n",
      "Epoch 401/1000, Loss: 1.0205\n",
      "Epoch 411/1000, Loss: 1.0118\n",
      "Epoch 421/1000, Loss: 1.0122\n",
      "Epoch 431/1000, Loss: 0.9904\n",
      "Epoch 441/1000, Loss: 0.9829\n",
      "Epoch 451/1000, Loss: 0.9901\n",
      "Epoch 461/1000, Loss: 0.9676\n",
      "Epoch 471/1000, Loss: 1.0313\n",
      "Epoch 481/1000, Loss: 0.9643\n",
      "Epoch 491/1000, Loss: 0.9773\n",
      "Epoch 501/1000, Loss: 0.9421\n",
      "Epoch 511/1000, Loss: 0.9365\n",
      "Epoch 521/1000, Loss: 0.9337\n",
      "Epoch 531/1000, Loss: 0.9311\n",
      "Epoch 541/1000, Loss: 0.9289\n",
      "Epoch 551/1000, Loss: 0.9267\n",
      "Epoch 561/1000, Loss: 0.9247\n",
      "Epoch 571/1000, Loss: 1.1728\n",
      "Epoch 581/1000, Loss: 1.0292\n",
      "Epoch 591/1000, Loss: 0.9898\n",
      "Epoch 601/1000, Loss: 0.9631\n",
      "Epoch 611/1000, Loss: 0.9268\n",
      "Epoch 621/1000, Loss: 0.9254\n",
      "Epoch 631/1000, Loss: 0.9240\n",
      "Epoch 641/1000, Loss: 0.9229\n",
      "Epoch 651/1000, Loss: 0.9221\n",
      "Epoch 661/1000, Loss: 0.9213\n",
      "Epoch 671/1000, Loss: 0.9206\n",
      "Epoch 681/1000, Loss: 0.9200\n",
      "Epoch 691/1000, Loss: 0.9195\n",
      "Epoch 701/1000, Loss: 1.0189\n",
      "Epoch 711/1000, Loss: 1.0851\n",
      "Epoch 721/1000, Loss: 1.1166\n",
      "Epoch 731/1000, Loss: 1.0514\n",
      "Epoch 741/1000, Loss: 0.9339\n",
      "Epoch 751/1000, Loss: 0.9604\n",
      "Epoch 761/1000, Loss: 0.9235\n",
      "Epoch 771/1000, Loss: 0.9568\n",
      "Epoch 781/1000, Loss: 0.9589\n",
      "Epoch 791/1000, Loss: 0.9510\n",
      "Epoch 801/1000, Loss: 0.9697\n",
      "Epoch 811/1000, Loss: 0.9568\n",
      "Epoch 821/1000, Loss: 1.4128\n",
      "Epoch 831/1000, Loss: 0.9633\n",
      "Epoch 841/1000, Loss: 0.9346\n",
      "Epoch 851/1000, Loss: 0.9710\n",
      "Epoch 861/1000, Loss: 0.9235\n",
      "Epoch 871/1000, Loss: 0.9828\n",
      "Epoch 881/1000, Loss: 0.9301\n",
      "Epoch 891/1000, Loss: 0.9224\n",
      "Epoch 901/1000, Loss: 0.9462\n",
      "Epoch 911/1000, Loss: 0.9209\n",
      "Epoch 921/1000, Loss: 0.9195\n",
      "Epoch 931/1000, Loss: 0.9192\n",
      "Epoch 941/1000, Loss: 0.9435\n",
      "Epoch 951/1000, Loss: 1.0226\n",
      "Epoch 961/1000, Loss: 0.9307\n",
      "Epoch 971/1000, Loss: 0.9236\n",
      "Epoch 981/1000, Loss: 0.9191\n",
      "Epoch 991/1000, Loss: 0.9188\n"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "model.train()\n",
    "for i in range(epoch):\n",
    "    total_loss=0\n",
    "    for x_train, y_train in train_dl:\n",
    "        xtrain, y_train = x_train.to(device), y_train.to(device)\n",
    "        out = model(x_train)\n",
    "        loss = loss_fn(out, y_train.argmax(dim=1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    writer.add_scalar(\"Loss/epoch\", avg_loss, i)\n",
    "\n",
    "    if (i % 10 == 0):\n",
    "        print(f\"Epoch {i+1}/{epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 0.8000\n",
      "Test Loss : 1.1061\n",
      "\n",
      "Total test samples :  20\n",
      "Correct predictions :  16\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Test set\n",
    "model.eval()\n",
    "tot_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_dl:\n",
    "        out = model(x_test)\n",
    "        y_test=y_test.argmax(dim=1)\n",
    "        loss = loss_fn(out, y_test)\n",
    "        tot_loss += loss.item()\n",
    "        y_pred = torch.argmax(out, dim=1)\n",
    "        correct += torch.sum(torch.eq(y_pred, y_test)).item()\n",
    "        total += y_test.shape[0]\n",
    "\n",
    "acc = correct / total\n",
    "loss = tot_loss / len(test_dl)\n",
    "print(f\"Test Acc {acc:.4f}\\nTest Loss : {loss:.4f}\\n\")\n",
    "print(\"Total test samples : \", total)\n",
    "print(\"Correct predictions : \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, \"swaram_lstm.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swaram-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
